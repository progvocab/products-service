
# Linear Regression 

## Loss functions 

### Mean Squared Error (MSE)
### Root Mean Squared Error (RMSE)
### Mean Absolute Error (MAE)
### Huber Loss
### Log-Cosh Loss
### Quantile Loss
### Mean Squared Logarithmic Error (MSLE)
### Poisson Loss
### Tweedie Loss
### Mean Absolute Percentage Error (MAPE)
### Symmetric Mean Absolute Percentage Error (SMAPE)


## Solver
### Normal Equation (closed-form)
### Batch Gradient Descent
### Stochastic Gradient Descent (SGD)
### Mini-Batch Gradient Descent
### Coordinate Descent
### Conjugate Gradient
### L-BFGS
### Newton’s Method
### Gauss-Newton
### Levenberg–Marquard

## Regularization
### Lasso Regression
**L1 regularization** that can shrink some coefficients to **exactly zero**, performing feature selection.
###  Ridge Regression 
**L2 regularization** that penalizes large coefficients to reduce overfitting and multicollinearity.


