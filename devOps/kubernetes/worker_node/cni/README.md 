Kubernetes CNI (Container Network Interface) is the plugin system that provides networking for Pods. It defines how Pod networks are created, assigned IPs, routed, and connected inside the cluster. Kubernetes itself does not create Pod networks â€” it delegates this job to a CNI plugin.

Below is a clear and complete explanation:


---

ğŸš€ What is CNI in Kubernetes?

CNI = Container Network Interface
It is a standard plugin interface that tells Kubernetes how to:

1. Create network namespaces for Pods


2. Attach interfaces to Pods (eth0)


3. Assign IP addresses


4. Configure routing


5. Clean up networking when Pods die



Kubernetes uses CNI plugins to implement the Pod-to-Pod networking model:

âœ” Every Pod gets its own IP
âœ” All Pods must be able to reach each other without NAT
âœ” Nodes must be able to reach all Pods
âœ” Pods must be able to reach external networks (via kube-proxy, NAT, etc.)


---

ğŸ§  Why do we need CNI?

Because Kubernetes doesn't include a built-in networking layer.

kubelet calls CNI plugins during Pod creation

the plugin sets up networking

kubelet then starts the container runtime (like containerd)


Without a CNI plugin, Pods cannot get IPs â†’ they stay in ContainerCreating.


---

ğŸ§© Examples of CNI Plugins

1. Flannel

Simple overlay network (VXLAN).
No network policy support.

2. Calico

BGP-based routing;
Supports NetworkPolicy (L3, L7, egress).

3. Cilium

Uses eBPF â†’ high performance, security & observability.

4. Weave Net

Mesh-based overlay; easy to install.

5. Amazon VPC CNI (AWS EKS)

Gives Pods IP directly from VPC â†’ deeply integrated with AWS.


---

ğŸ› ï¸ How CNI Works (Step-by-Step)

1. Pod starts

kubelet tells container runtime to create a Pod.

2. Container runtime calls CNI

CRI (like containerd) triggers CNI ADD event:

CNI_ADD(containerID, networkNamespace, podName, nodeName)

3. CNI plugin performs actions

Creates veth pairs

Assigns Pod its IP address

Configures routes

Configures bridge/overlay/BPF rules

Updates IPAM (IP Address Management)


Pod now has:

eth0 --> Pod Namespace
veth0 --> Host Namespace (attached to bridge or overlay)

4. kube-proxy / CNI routing handle traffic

Depending on plugin:

Flannel â†’ VXLAN overlay

Calico â†’ BGP

Cilium â†’ eBPF dataplane

AWS VPC CNI â†’ Pods get ENIs/IPs directly from AWS VPC


5. When Pod dies

CNI receives DEL event â†’ cleans up veth, routes, IP allocation.


---

ğŸ•¸ï¸ CNI vs. kube-proxy vs. Service Mesh

Component	Role

CNI	Creates Pod network, routes traffic between Pods
kube-proxy	Implements Kubernetes Services & ClusterIPs (iptables/ipvs/BPF)
Service Mesh (Istio, Linkerd)	Layer 7 features like mTLS, retries, routing



---

ğŸ” Visual Summary

Pod Network Namespace
---------------------
eth0 (Pod IP)
   |
veth pair
   |
Host Namespace
   |
CNI Plugin (bridge / overlay / eBPF)
   |
Node Network Interface (eth0)
   |
Cluster Network / Cloud Network


---

ğŸ“Œ Key Takeaways

Kubernetes does not create Pod networking â†’ CNI does.

CNI handles IP allocation, routing, interfaces, cleanup.

kubelet calls CNI on Pod create/delete.

CNI plugins differ in performance & features.

AWS, GCP, Azure each have their own optimized CNIs.



---

If you'd like, I can also explain:

ğŸ‘‰ How Pod networking is built using veth pairs
ğŸ‘‰ How CNIs differ (Calico vs Cilium vs AWS VPC CNI)
ğŸ‘‰ How Network Policies work
ğŸ‘‰ Draw CNI Networking diagrams
Just tell me!

CNI plugins run on every worker node, not on the control plane.

Here is the clear breakdown:


---

âœ… Where does CNI run?

â¡ï¸ CNI binaries and configuration live on each worker node

Each node (including control-plane nodes if they run Pods) has:

/opt/cni/bin/* â†’ CNI plugin binaries

/etc/cni/net.d/* â†’ CNI configuration

A CNI daemonset Pod (Calico, Cilium, Flannel, etc.)


Why?

Because Pod network setup happens on the node where the Pod runs, and kubelet on that node must call the CNI plugin locally.


---

ğŸ§  How it works on worker nodes

When a Pod is created on Node X:

1. kubelet on Node X calls CNI ADD


2. CNI plugin on Node X:

creates veth pair

assigns Pod IP

configures routing / overlay / BPF



3. Pod starts running on Node X with networking ready



So CNI is node-local, not centralized.


---

â— Control plane nodes

Control plane nodes do not run the CNI for the cluster.

But:

If your control-plane is also a worker (in small clusters like kubeadm) â†’ they also run CNI.

In managed clusters (EKS, GKE, AKS), control-plane nodes are isolated â†’ only workers run CNI.



---

ğŸ“Œ Summary

Node Type	CNI Present?	Why

Worker Node	âœ” Yes	Needed to configure Pod networking
Control Plane Node (no Pods)	âœ– No	No Pod networking happens here
Control Plane Node that schedules Pods	âœ” Yes	Still acts like a worker node



---

If you want, I can explain what exactly runs on the worker node, how CNI interacts with kubelet, and how CNI pods run as a DaemonSet.


Here is the cleanest, most accurate explanation of how CNI interacts with kube-proxy, and where kube-proxy runs.


---

ğŸ§© Part 1: Where does kube-proxy run?

kube-proxy runs on every worker node.

âœ” It is deployed as a DaemonSet
âœ” Each node gets one kube-proxy instance
âœ” Even if the control-plane node is also a worker, kube-proxy runs there too

Control-plane nodes DO NOT run kube-proxy in managed clusters (EKS, GKE, AKS) because they never host Pods.


---

ğŸ§© Part 2: How CNI interacts with kube-proxy?

CNI and kube-proxy do not talk directly.
Instead, they influence different parts of the data path.

Hereâ€™s the real relationship:

POD NETWORKING (L3)
--------------------
â¬† CNI plugin sets:
- Pod IP
- veth pair
- routing
- overlay/BGP/BPF dataplane

SERVICE NETWORKING (L4)
-----------------------
â¬† kube-proxy sets:
- iptables/ipvs/bpf rules
- ClusterIP load balancing

âœ” CNI is responsible for:

Pod IP allocation

Pod-to-Pod routing

Node-to-Pod routing

Overlay networking (if used)

eBPF dataplane (Cilium/Calico eBPF mode)


âœ” kube-proxy is responsible for:

Implementing Kubernetes Services

Managing iptables / IPVS / eBPF service rules

ClusterIP

NodePort

ExternalTrafficPolicy

LoadBalancer traffic



---

ğŸ§  How they work together (indirectly)

Step 1 â€” CNI sets up Pod networking

When a Pod starts:

kubelet â†’ calls CNI ADD

CNI creates veth connections

assigns Pod IP

configures routing


At this point: âœ” Pod can reach other Pods (via CNI routing)

But: âœ– Services are NOT handled by CNI


---

Step 2 â€” kube-proxy picks up the Service/Endpoint info

kube-proxy watches:

Services

Endpoints (Pod IPs)


From the K8s API server.

Then it programs:

iptables

or IPVS

or eBPF map (Cilium without kube-proxy)


Example for ClusterIP:

iptables rule:
Service IP â†’ Pod IP targets

Now: âœ” Pod can use ClusterIP Services
âœ” NodePort works
âœ” LoadBalancer traffic is forwarded


---

ğŸ§© Who handles what?

CNI handles:

Pod routing

Pod IP assignment

Pod network namespace

Overlay network (Flannel, Calico, Cilium)


kube-proxy handles:

ClusterIP

NodePort

LoadBalancer rules

DNAT (for service load balancing)


They operate on different layers:

CNI = Layer 2/Layer 3 networking  
kube-proxy = Layer 4 service load-balancing


---

ğŸ’¡ Special case: Cilium eBPF mode (no kube-proxy)

Some CNIs (Cilium, Calico eBPF mode) replace kube-proxy:

They implement services using eBPF

kube-proxy DaemonSet is disabled

Service routing is handled inside the kernel dataplane


But the architecture is still:

CNI = pod networking

L4 service load-balancing = BPF service engine



---

ğŸ“Œ Final Summary

Component	Runs Where?	Does What?

CNI plugin	Worker nodes	Pod IPs, routing, veth, overlay
kube-proxy	Worker nodes	Implements Services (ClusterIP, NodePort)
Control Plane nodes	No kube-proxy	Only API server / scheduler / controller



---

If you want, I can also explain:

ğŸ‘‰ Flow of a packet from Pod â†’ Pod â†’ Service â†’ External
ğŸ‘‰ How kube-proxy rules look in iptables
ğŸ‘‰ How Cilium replaces kube-proxy using eBPF
ğŸ‘‰ Diagrams of CNI + kube-proxy data path

Just tell me!

Here is the clear, correct architecture for how CNI interacts with LoadBalancers, and where the Load Balancer actually lives in Kubernetes.


---

ğŸ§© First: Where is the Load Balancer in Kubernetes?

Kubernetes does NOT run a Load Balancer on worker or control-plane nodes.

When you create a Service of type LoadBalancer, Kubernetes does this:

âœ” In cloud providers (AWS, GCP, Azure):

A cloud Load Balancer is created outside the cluster

AWS â†’ ELB / NLB

GCP â†’ GCLB

Azure â†’ Azure LB



âœ” In on-prem clusters:

You must run something like MetalLB

MetalLB runs on worker nodes and assigns IPs using ARP/BGP


So:

Load Balancer is NOT inside the cluster.

Not on control-plane

Not on worker nodes

Kubernetes just configures an external LB



---

ğŸ§© How LoadBalancer traffic flows

Example: AWS NLB

Internet
   â†“
AWS Load Balancer (external)
   â†“
Worker Node (NodePort)
   â†“
kube-proxy
   â†“
Pod IP (via CNI routing)


---

ğŸ§© Part 2: How CNI interacts with Load Balancers?

âš ï¸ Important:

CNI does NOT talk to Load Balancers and does NOT configure them.

They operate on completely different layers.

So what does CNI do?

CNI ensures the Pod has:

a network namespace

an IP

routing to other Pods

routes back to the node

veth pair connected to the node


This allows a Load Balancer â†’ Node â†’ Pod path to work.
It ensures:

âœ” Node can forward traffic to Pod IP
âœ” Pod IP is reachable inside the cluster
âœ” Return packets flow back properly

What part does the Load Balancer interact with?

It sends traffic to NodePort ports on worker nodes.

Then kube-proxy receives and forwards the traffic.

Finally CNI ensures the Pod networking works internally.



---

ğŸ§© Interaction Diagram

External LB
         â†“
   [Worker Node]
         â†“
  kube-proxy (NodePort rules)
         â†“
   Pod IP (CNI routing)
         â†“
         Pod

Only kube-proxy interacts with LoadBalancer â†’ NOT CNI

CNI only ensures Pod networking.


---

ğŸ§  Putting it all together

Component	Where it runs	Responsible for

External Load Balancer	Outside cluster	Sends traffic to NodePort on worker nodes
kube-proxy	Worker nodes	Converts LoadBalancer â†’ NodePort â†’ PodIP
CNI plugin	Worker nodes	Pod networking (IP, veth, routing)



---

ğŸ“Œ Final Summary (simple)

Load Balancer lives outside the cluster.

kube-proxy handles NodePort + Service routing.

CNI only handles Pod IP networking, not Load Balancers.

Load Balancer traffic â†’ goes to worker nodes â†’ kube-proxy â†’ Pod via CNI routes.



---

If you want, I can also explain:

ğŸ‘‰ Exactly how AWS NLB â†’ NodePort mapping works
ğŸ‘‰ Packet flow diagram NodePort â†’ PodIP
ğŸ‘‰ How MetalLB works on-prem
ğŸ‘‰ How Cilium replaces kube-proxy for LoadBalancers

Just tell me!


Envoy is used in Kubernetes as a high-performance Layer 4/Layer 7 proxy.
Its biggest role is inside a service mesh, where it provides traffic control, security, and observability for microservices.

Letâ€™s break it down clearly.


---

âœ… What is Envoy?

Envoy is a sidecar proxy (written in C++) that sits next to each Pod, intercepting all inbound and outbound traffic.

It is NOT the CNI, NOT kube-proxy, and NOT the load balancer.

It is an L4/L7 proxy used for advanced traffic features.


---

ğŸ§© Why Envoy is used inside Kubernetes?

âœ” 1. Service Mesh Sidecar Proxy (Istio, Consul, Kuma, AWS App Mesh)

Most commonly, Envoy is deployed as a sidecar container in each Pod:

Pod:
 â”œâ”€â”€ Your App Container
 â””â”€â”€ Envoy Sidecar

It handles all the networking concerns for your application, such as:

mTLS encryption

retries

timeouts

load balancing

circuit breaking

request routing

header-based routing

traffic shadowing

observability (metrics, logs, traces)



---

âœ” 2. Traffic Management (L7 Smart Routing)

Because Envoy works at Layer 7, it can route traffic based on:

HTTP headers

User agents

URLs

gRPC methods

Version-based routing (canary, blue/green)

A/B testing


This is NOT possible with CNI or kube-proxy.


---

âœ” 3. Security (Zero-Trust Architecture)

Envoy enables:

Mutual TLS (mTLS) between services

Certificate rotation

Identity-based authorization

RBAC and policy enforcement

Encryption in transit



---

âœ” 4. Observability and Telemetry

Envoy collects:

Request metrics (latency, RPS, errors)

Distributed tracing info (Jaeger, Zipkin)

Access logs

Connection statistics


This helps teams monitor microservice traffic without changing application code.


---

âœ” 5. Ingress Gateway (Envoy as Ingress Controller)

Envoy can also run as an ingress gateway:

Internet
   â†“
Envoy Ingress Gateway
   â†“
Service / Pods

For example:

Istio Ingress Gateway

Ambassador API Gateway

Gloo Edge
all use Envoy under the hood.



---

ğŸ§  Important: What Envoy is not

Component	Does Envoy replace it?	Explanation

kube-proxy	âŒ No	kube-proxy does ClusterIP/NodePort routing (L4)
CNI plugin	âŒ No	CNI handles Pod IPs, routing, networking namespace
Cloud Load Balancer	âŒ No	Envoy does not replace NLB/ELB/ALB
Ingress Controller	âœ” Sometimes	Some ingress controllers are built using Envoy



---

ğŸ§© Where Envoy sits in the Kubernetes data path?

Client
  â†“
(Cloud Load Balancer)
  â†“
Envoy Ingress Gateway (optional)
  â†“
Envoy Sidecar in each Pod
  â†“
Application Container

Envoy handles application-level traffic rules, not cluster networking.


---

ğŸ“Œ Final Summary

Envoy is used in Kubernetes for:

âœ” service mesh sidecar

âœ” mTLS and security

âœ” smart routing (canary, A/B)

âœ” retries, circuit breaking

âœ” rich telemetry

âœ” ingress gateway proxy

It does not replace CNI, kube-proxy, or LoadBalancer.


---

If you want, I can also explain:

ğŸ‘‰ How Envoy intercepts traffic using iptables
ğŸ‘‰ Envoy vs NGINX vs HAProxy in Kubernetes
ğŸ‘‰ Complete data path with Istio (diagram)
ğŸ‘‰ How Envoy sidecar is injected into Pods

Just tell me!

Ingress and CNI never directly interact with each other.
But they depend on each other indirectly in the traffic path.

Here is the clean, correct explanation.


---

ğŸ§© What Ingress Actually Is

An Ingress is just a Kubernetes API object that defines L7 routing rules.

Example:

/api â†’ service1
/web â†’ service2

To actually handle traffic, you need an Ingress Controller (NGINX, Envoy, HAProxy, Traefik, Istio Gateway).

This controller runs as a Pod on a worker node.


---

ğŸ§© Where CNI fits

CNI is responsible for:

Pod IP assignment

routing Pod â†” Pod

network namespaces

veth pair + bridge/overlay/BGP/eBPF setup


CNI works at Layer 2 / Layer 3.


---

ğŸ§© Where Ingress Controller fits

Ingress controller:

Listens on NodePort / LoadBalancer

Forwards traffic to Services

Does HTTP/HTTPS routing

Works at Layer 7



---

ğŸ§© So how do they â€œinteractâ€? (Indirect Path)

Although CNI and ingress do not talk directly, CNI provides the network plumbing that ingress needs.

Letâ€™s walk through the packet path:


---

ğŸ“¦ Request flow

1. External Client â†’ Load Balancer

(If you're using Service type LoadBalancer)

Internet
   â†“
Cloud Load Balancer (AWS ELB/NLB, etc.)

2. LB â†’ Worker Node

LB forwards traffic to NodePort of the Ingress Controller Pod.

LoadBalancer â†’ NodePort â†’ kube-proxy

3. kube-proxy â†’ Ingress Controller Pod (via CNI)

kube-proxy forwards traffic to the Pod IP of the Ingress Controller.

Now CNI handles routing:

Node Namespace 
   â†“ (CNI routing)
Ingress Controller Pod (Pod IP)

4. Ingress Controller â†’ Backend Pods (via CNI)

Ingress controller now forwards traffic to the backend service.

Example:

Ingress â†’ Service â†’ Endpoint (Pod IP)

CNI again handles:

Pod-to-Pod routing

veth/overlay/BGP/BPF network path



---

ğŸ§© Key Point: Indirect Interaction

Ingress needs CNI, but Ingress does not call CNI.

Why?

Because:

CNI enables Pods to have IP addresses

CNI enables routing between Pods

Ingress Controller Pod is just another Pod â†’ depends on CNI networking

All backend Pods also depend on CNI networking


Thus the â€œinteractionâ€ is really:
CNI provides the network. Ingress uses the network.


---

ğŸ§  Summary Table

Component	Layer	Role	Interaction with CNI

CNI	L2/L3	Pod networking	Provides Pod IP + routing
Ingress Controller	L7	HTTP/HTTPS routing	Uses CNI to reach Pods
kube-proxy	L4	Service routing	Sits between CNI & Ingress



---

ğŸ–¼ï¸ Full traffic path diagram

Internet
   â†“
External Load Balancer (AWS/GCP/Azure)
   â†“
Worker Node (NodePort)
   â†“
kube-proxy (iptables/ipvs/bpf)
   â†“
Ingress Controller Pod  â†--- CNI creates Pod IP + routing
   â†“
Service (ClusterIP)
   â†“
kube-proxy
   â†“
Backend Pod  â†--- CNI routes traffic to Pod IP


---

If you want, I can also explain:

ğŸ‘‰ How Ingress works internally (NGINX, Envoy, HAProxy)
ğŸ‘‰ How Ingress Controller communicates with kube-proxy
ğŸ‘‰ Full diagram of CNI + Ingress + kube-proxy packets
ğŸ‘‰ How Istio Gateway differs from Ingress

Just tell me!